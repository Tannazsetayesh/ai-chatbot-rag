{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Setup**"
      ],
      "metadata": {
        "id": "amU72N75X85m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependnecies"
      ],
      "metadata": {
        "id": "dyV2zI7Ibxf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-storage pinecone-client tqdm pandas pinecone google-cloud-aiplatform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "525rs9dqlSJD",
        "outputId": "42bc6d12-4d1e-4fbd-d334-bec302c9fa99"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (7.2.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.6.15)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.14.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (1.7.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (3.34.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.20.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.11.7)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (4.9.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP Permissions"
      ],
      "metadata": {
        "id": "UR_iQ8uyL87J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gcloud projects add-iam-policy-binding vertex-ai-rag-463204 --member=\"user:tanaz.setayesh@gmail.com\" --role=\"roles/aiplatform.user\"\n",
        "# !gcloud projects add-iam-policy-binding vertex-ai-rag-463204 --member=\"user:tanaz.setayesh@gmail.com\" --role=\"roles/aiplatform.viewer\""
      ],
      "metadata": {
        "id": "_3rWojz_L8oW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "sELuidyW3_MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import uuid\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from datetime import datetime\n",
        "from pinecone import Pinecone\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import userdata\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "from google.cloud.aiplatform_v1 import PredictionServiceClient\n",
        "#from google.cloud.aiplatform_v1 import ResourceExhausted\n",
        "\n",
        "from google.protobuf.struct_pb2 import Struct\n",
        "from google.api_core.exceptions import ResourceExhausted, NotFound, PermissionDenied\n",
        "\n",
        "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n"
      ],
      "metadata": {
        "id": "fnuShV7Y39ul"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Google Cloud Storage configuration\n",
        "BUCKET_NAME = \"anika-chatbot\"\n",
        "GCS_PREFIX = \"documents_\"\n",
        "\n",
        "# Authenticate in Colab\n",
        "auth.authenticate_user()\n",
        "print('GCP user is Authenticated.')\n",
        "\n",
        "# Pinecone configuration\n",
        "PINECONE_API_KEY=userdata.get('PINECONE_API_KEY')\n",
        "PINECONE_HOST = \"https://irs-chatbot-index-3c8vbcs.svc.aped-4627-b74a.pinecone.io\"\n",
        "NAMESPACE = \"irs-namespace\"\n",
        "\n",
        "print('PINECONE config loaded.')\n",
        "\n",
        "\n",
        "# Vertex AI configuration\n",
        "PROJECT_ID = \"vertex-ai-rag-463204\"\n",
        "LOCATION = \"us-central1\"\n",
        "EMBEDDING_MODEL = \"text-embedding-005\"\n",
        "FALLBACK_EMBEDDING_MODEL = \"text-embedding-004\"\n",
        "LLM_MODEL = \"gemini-2.0-flash\"  # Primary LLM\n",
        "FALLBACK_LLM_MODEL = \"text-bison\"\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6c3pfuUunIH",
        "outputId": "32129e3c-0cd6-4e2a-a967-1ca2abf86929"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCP user is Authenticated.\n",
            "PINECONE config loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wipe_pinecone_index(api_key=PINECONE_API_KEY, host=PINECONE_HOST, namespace=NAMESPACE):\n",
        "    \"\"\"\n",
        "    Deletes all vectors from a Pinecone serverless index.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Your Pinecone API key.\n",
        "        host (str): The host URL of your Pinecone index.\n",
        "        namespace (str, optional): The namespace to clear. Defaults to 'default' if not specified.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pc = Pinecone(api_key=api_key)\n",
        "        index = pc.Index(host=host)\n",
        "        print(\"‚úÖ Pinecone client initialized successfully\")\n",
        "\n",
        "        # Print current index stats\n",
        "        stats = index.describe_index_stats()\n",
        "        print(\"üìä Current index stats:\", stats)\n",
        "\n",
        "        # Delete vectors\n",
        "        if namespace:\n",
        "            index.delete(delete_all=True, namespace=namespace)\n",
        "            print(f\"üßπ All data deleted from namespace '{namespace}'\")\n",
        "        else:\n",
        "            index.delete(delete_all=True)\n",
        "            print(\"üßπ All data deleted from the default namespace\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to wipe Pinecone index: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "X5y7duuksCZ7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read user data**"
      ],
      "metadata": {
        "id": "pkCrcFqfb9TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select & Acquire The Dataset"
      ],
      "metadata": {
        "id": "kY_c55aZQxpc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2bwAJU4sdltL"
      },
      "outputs": [],
      "source": [
        "def download_html_to_gcs(start_url=\"https://www.irs.gov/irm/part1\", bucket_name=\"anika-chatbot\"):\n",
        "\n",
        "    print(f\"Starting download from: {start_url}\")\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    start_url_parsed = urlparse(start_url)\n",
        "    start_url_base = start_url_parsed.scheme + \"://\" + start_url_parsed.netloc + start_url_parsed.path\n",
        "\n",
        "    def get_html(url):\n",
        "        \"\"\"Fetches HTML content from a URL.\"\"\"\n",
        "        try:\n",
        "            print(f\"Fetching URL: {url}\")\n",
        "            response = requests.get(url, timeout=10) # Added timeout\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "            return response.text\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def upload_to_gcs(html_content, blob_name):\n",
        "        \"\"\"Uploads HTML content to GCS.\"\"\"\n",
        "        try:\n",
        "            print(f\"Uploading to GCS blob: {blob_name}\")\n",
        "            blob = bucket.blob(blob_name)\n",
        "            blob.upload_from_string(html_content, content_type='text/html')\n",
        "            print(f\"Successfully uploaded {blob_name}\")\n",
        "            return True # Indicate successful upload\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading to GCS blob {blob_name}: {e}\")\n",
        "            return False # Indicate failed upload\n",
        "\n",
        "\n",
        "    visited_urls = set()\n",
        "    # Remove fragment from start_url before adding to queue and visited set\n",
        "    urls_to_visit = [(start_url_base, 0)] # Store (url, level)\n",
        "    visited_urls.add(start_url_base)\n",
        "\n",
        "    files_downloaded_count = 0 # Initialize counter\n",
        "\n",
        "    while urls_to_visit and files_downloaded_count < 100: # Add condition for file count\n",
        "        current_url, level = urls_to_visit.pop(0) # Use pop(0) for breadth-first\n",
        "\n",
        "        # No need to check visited_urls here as we add to it when adding to urls_to_visit\n",
        "        if level > 2:\n",
        "            continue\n",
        "\n",
        "        html_content = get_html(current_url)\n",
        "        if html_content:\n",
        "            # Create a blob name from the URL\n",
        "            blob_name = current_url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\") + \".html\"\n",
        "            if upload_to_gcs(html_content, blob_name): # Check if upload was successful\n",
        "                files_downloaded_count += 1 # Increment counter only on successful upload\n",
        "\n",
        "            if level < 2 and files_downloaded_count < 100: # Add condition for file count\n",
        "                soup = BeautifulSoup(html_content, 'html.parser')\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link.get('href')\n",
        "                    if href:\n",
        "                        # Resolve relative URLs\n",
        "                        new_url = urljoin(current_url, href)\n",
        "\n",
        "                        # Remove fragment from the new_url for comparison and storage\n",
        "                        new_url_parsed = urlparse(new_url)\n",
        "                        new_url_base = new_url_parsed.scheme + \"://\" + new_url_parsed.netloc + new_url_parsed.path\n",
        "\n",
        "                        # Check if the new URL is within the same domain AND is a subpath of the start_url\n",
        "                        if urlparse(new_url_base).netloc == urlparse(start_url_base).netloc and new_url_base.startswith(start_url_base):\n",
        "                            if new_url_base not in visited_urls:\n",
        "                                visited_urls.add(new_url_base) # Add to visited set\n",
        "                                urls_to_visit.append((new_url_base, level + 1))\n",
        "                                print(f\"Adding {new_url_base} to visit queue at level {level + 1}\")\n",
        "\n",
        "        if files_downloaded_count >= 100:\n",
        "            print(\"Reached maximum number of files to download (100). Stopping.\")\n",
        "            break # Exit the loop if max files downloaded\n",
        "\n",
        "        time.sleep(1) # Add a small delay to avoid overwhelming servers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare & Extract Text"
      ],
      "metadata": {
        "id": "m5xiS17JV6pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read all HTML fiels and create single json file with Title and Text\n",
        "def process_html_from_gcs(bucket_name=\"anika-chatbot\"):\n",
        "\n",
        "    print(f\"Processing HTML files from bucket: {bucket_name}\")\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "    # List of tags to remove\n",
        "    tags_to_remove = [\n",
        "        \"script\",\n",
        "        \"style\",\n",
        "        \"iframe\",\n",
        "        \"noscript\",\n",
        "        \"header\",\n",
        "        \"footer\",\n",
        "        \"nav\",\n",
        "        \"link\",\n",
        "        \"meta\",\n",
        "        \"form\",\n",
        "        \"input\",\n",
        "        \"button\",\n",
        "        \"select\",\n",
        "        \"svg\",\n",
        "    ]\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    # List blobs in the bucket that end with .html\n",
        "    blobs = bucket.list_blobs(prefix=\"\") # List all blobs, we'll filter later\n",
        "\n",
        "    for blob in blobs:\n",
        "        if blob.name.endswith(\".html\"):\n",
        "            print(f\"Reading file: {blob.name}\")\n",
        "            try:\n",
        "                # Download the blob content as a string\n",
        "                html_content = blob.download_as_text()\n",
        "\n",
        "                # Parse the HTML content\n",
        "                soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "                # Remove specified tags\n",
        "                for tag in tags_to_remove:\n",
        "                    for element in soup.find_all(tag):\n",
        "                        element.extract()\n",
        "\n",
        "                # Extract text from the remaining content\n",
        "                text = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "                # Get the title (if available in the HTML)\n",
        "                title = soup.title.string if soup.title else blob.name.replace(\".html\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "                documents.append({'title': title, 'text': text})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {blob.name}: {e}\")\n",
        "\n",
        "    # Write the documents data structure to a JSON file in GCS\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    output_blob_name = f\"documents_{timestamp}.json\"\n",
        "\n",
        "    try:\n",
        "        output_blob = bucket.blob(output_blob_name)\n",
        "        output_blob.upload_from_string(json.dumps(documents), content_type='application/json')\n",
        "        print(f\"Successfully wrote extracted data to {output_blob_name} in bucket {bucket_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing documents to GCS {output_blob_name}: {e}\")\n",
        "\n",
        "# process_html_from_gcs()"
      ],
      "metadata": {
        "id": "maQ0YJlRdtVv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding & Indexing**"
      ],
      "metadata": {
        "id": "t1N642wZYNnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pincone initializing"
      ],
      "metadata": {
        "id": "d6T8aynMZEVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Pinecone client\n",
        "try:\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    index = pc.Index(host=PINECONE_HOST)\n",
        "    print(\"Pinecone client initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Pinecone client: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o28SQcSdE_B7",
        "outputId": "926b41d2-c295-44e9-8ab4-310dfaa59e02"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone client initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Cleaned Documents from Cloud Storage"
      ],
      "metadata": {
        "id": "VK4jJ6DH4RBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to read JSON from GCS\n",
        "def read_json_from_gcs(bucket_name=BUCKET_NAME, prefix=GCS_PREFIX):\n",
        "    \"\"\"\n",
        "    Read JSON files from GCS bucket that match the given prefix.\n",
        "    Returns a list of JSON objects.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blobs = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "        all_documents = []\n",
        "        for blob in tqdm(list(blobs), desc=\"Reading GCS files\"):\n",
        "            if blob.name.endswith(\".json\"):\n",
        "                print(f\"Processing file: {blob.name}\")\n",
        "                json_data = blob.download_as_text()\n",
        "                documents = json.loads(json_data)\n",
        "                all_documents.extend(documents)\n",
        "\n",
        "        print(f\"Retrieved {len(all_documents)} documents from GCS\")\n",
        "        return all_documents\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading from GCS: {e}\")\n",
        "        raise\n",
        "\n",
        "#temp=read_json_from_gcs()"
      ],
      "metadata": {
        "id": "oFszaviIF3uz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking Text for Embedding"
      ],
      "metadata": {
        "id": "2MvKFfLfkl3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text chunking function\n",
        "def chunk_text(text, max_chunk_size=500):\n",
        "    \"\"\"\n",
        "    Split text into chunks of approximately max_chunk_size characters.\n",
        "    Attempts to split at sentence boundaries to preserve context.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
        "                current_chunk += sentence + \" \"\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence + \" \"\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "        print(f\"Chunked text into {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error chunking text: {e}\")\n",
        "        return [text]  # Fallback to single chunk if error occurs"
      ],
      "metadata": {
        "id": "_kbx41pdmF91"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Pinecone records"
      ],
      "metadata": {
        "id": "cULrPR5YJpYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to prepare Pinecone records\n",
        "def prepare_records(documents):\n",
        "    \"\"\"\n",
        "    Prepare records for Pinecone upsert from JSON documents.\n",
        "    Chunks large texts and generates unique IDs.\n",
        "    Ensures records have a valid 'text' field and metadata fields are flat and of valid types.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        records = []\n",
        "        for doc in tqdm(documents, desc=\"Preparing records\"):\n",
        "            title = doc.get(\"title\", \"\")\n",
        "            text_content = doc.get(\"text\", \"\")\n",
        "\n",
        "            # Skip documents with empty text\n",
        "            if not text_content.strip():\n",
        "                print(f\"Skipping document with empty text: title='{title}'\")\n",
        "                continue\n",
        "\n",
        "            # Chunk the text if it's too long\n",
        "            chunks = chunk_text(text_content, max_chunk_size=500)\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                # Validate chunk is not empty\n",
        "                if not chunk.strip():\n",
        "                    print(f\"Skipping empty chunk {i} for document: title='{title}'\")\n",
        "                    continue\n",
        "\n",
        "                # Prepare metadata fields (flattened, no nested dictionary)\n",
        "                metadata = {\n",
        "                    \"title\": str(title),  # Ensure string type\n",
        "                    \"chunk_index\": i,  # Integer is valid\n",
        "                }\n",
        "\n",
        "                # Validate metadata types\n",
        "                for key, value in metadata.items():\n",
        "                    if not isinstance(value, (str, int, float, bool, list)):\n",
        "                        print(f\"Invalid metadata type for {key}: {type(value)}. Converting to string.\")\n",
        "                        metadata[key] = str(value)\n",
        "                    if isinstance(value, list) and not all(isinstance(item, str) for item in value):\n",
        "                        print(f\"Invalid list type for {key}. Converting to string.\")\n",
        "                        metadata[key] = str(value)\n",
        "\n",
        "                record = {\n",
        "                    \"_id\": str(uuid.uuid4()),  # Unique ID for each chunk\n",
        "                    \"text\": chunk,  # Text field for embedding\n",
        "                    \"category\": \"irs_info\",  # Customize as needed\n",
        "                    **metadata  # Flatten metadata into top-level fields\n",
        "                }\n",
        "                records.append(record)\n",
        "\n",
        "        if records:\n",
        "            print(f\"Sample record: {records[0]}\")\n",
        "\n",
        "        print(f\"Prepared {len(records)} valid records for upsert (skipped {len(documents) * len(chunks) - len(records)} invalid chunks)\")\n",
        "        return records\n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing records: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "PRgH7_eWGCfU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed Data using vertex"
      ],
      "metadata": {
        "id": "k3FN6idXmkKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_documents(documents, project_id, location, model_name, fallback_model=\"text-embedding-004\", task_type=\"RETRIEVAL_DOCUMENT\", batch_size=50):\n",
        "    \"\"\"\n",
        "    Generate embeddings for document chunks using Vertex AI TextEmbeddingModel.\n",
        "\n",
        "    Args:\n",
        "        documents: List of dictionaries containing 'text' field with document chunks.\n",
        "        project_id: Google Cloud project ID.\n",
        "        location: Vertex AI region.\n",
        "        model_name: Primary embedding model (e.g., text-embedding-005).\n",
        "        fallback_model: Fallback embedding model (e.g., text-embedding-004).\n",
        "        task_type: Embedding task type (default: RETRIEVAL_DOCUMENT).\n",
        "        batch_size: Number of documents to embed per batch (default: 50).\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with original document data plus 'embedding' field.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        embedded_documents = []\n",
        "        for model in [model_name, fallback_model]:\n",
        "            try:\n",
        "                embedding_model = TextEmbeddingModel.from_pretrained(model)\n",
        "                print(f\"Using embedding model: {model}\")\n",
        "\n",
        "                for i in tqdm(range(0, len(documents), batch_size), desc=f\"Embedding documents with {model}\"):\n",
        "                    batch = documents[i:i + batch_size]\n",
        "                    texts = [doc.get(\"text\", \"\") for doc in batch]\n",
        "                    # Skip empty texts\n",
        "                    valid_texts = [text for text in texts if text.strip()]\n",
        "                    if not valid_texts:\n",
        "                        print(f\"Skipping batch {i // batch_size + 1}: No valid texts\")\n",
        "                        continue\n",
        "\n",
        "                    inputs = [TextEmbeddingInput(text=text, task_type=task_type) for text in valid_texts]\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    for attempt in range(3):\n",
        "                        try:\n",
        "                            response = embedding_model.get_embeddings(inputs)\n",
        "                            embeddings = [r.values for r in response]\n",
        "\n",
        "                            # Validate embedding dimensions\n",
        "                            for emb in embeddings:\n",
        "                                if len(emb) != 768:\n",
        "                                    raise ValueError(f\"Expected 768 dimensions, got {len(emb)}\")\n",
        "\n",
        "                            # Map embeddings back to documents\n",
        "                            valid_idx = 0\n",
        "                            for doc, text in zip(batch, texts):\n",
        "                                if text.strip():\n",
        "                                    embedded_doc = doc.copy()\n",
        "                                    embedded_doc[\"embedding\"] = embeddings[valid_idx]\n",
        "                                    embedded_documents.append(embedded_doc)\n",
        "                                    valid_idx += 1\n",
        "                                else:\n",
        "                                    print(f\"Skipping empty document: {doc.get('title', 'unknown')}\")\n",
        "\n",
        "                            elapsed_time = time.time() - start_time\n",
        "                            print(f\"Embedded batch {i // batch_size + 1} with {len(valid_texts)} documents \"\n",
        "                                  f\"in {elapsed_time:.2f}s\")\n",
        "                            break\n",
        "\n",
        "                        except ResourceExhausted as e:\n",
        "                            wait_time = (2 ** attempt) * 10\n",
        "                            print(f\"Vertex AI rate limit hit for {model}: {e}. Waiting {wait_time}s.\")\n",
        "                            time.sleep(wait_time)\n",
        "                            if attempt == 2:\n",
        "                                raise\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error embedding batch {i // batch_size + 1} with {model}: {e}\")\n",
        "                            raise\n",
        "\n",
        "                    # Add delay to avoid overwhelming API\n",
        "                    if i + batch_size < len(documents):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                print(f\"Successfully embedded {len(embedded_documents)} documents with {model}\")\n",
        "                return embedded_documents\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed with model {model}: {e}\")\n",
        "                if model == fallback_model:\n",
        "                    raise\n",
        "                continue\n",
        "\n",
        "        raise ValueError(f\"Both models {model_name} and {fallback_model} failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to embed documents: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "FfhC16-lmqAz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsert (insert/update) to pinecone\n"
      ],
      "metadata": {
        "id": "A8du3lE44yqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to upsert to Pinecone\n",
        "\n",
        "def upsert_to_pinecone(records, namespace, tokens_per_record=250, token_limit_per_minute=250000):\n",
        "    \"\"\"\n",
        "    Upsert records to Pinecone index in batches with rate-limiting.\n",
        "    Records must include 'embedding' field with precomputed vectors.\n",
        "    Args:\n",
        "        records: List of records with '_id', 'text', 'embedding', and metadata fields.\n",
        "        namespace: Pinecone namespace.\n",
        "        tokens_per_record: Estimated tokens per record (default: 250).\n",
        "        token_limit_per_minute: Token limit per minute (default: 250,000).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate records\n",
        "        for record in records:\n",
        "            if \"text\" not in record:\n",
        "                raise ValueError(f\"Record missing 'text' field: {record}\")\n",
        "            if \"embedding\" not in record:\n",
        "                raise ValueError(f\"Record missing 'embedding' field: {record}\")\n",
        "            if not isinstance(record.get(\"text\"), str):\n",
        "                raise ValueError(f\"Invalid 'text' type: {type(record.get('text'))}\")\n",
        "            if not isinstance(record.get(\"embedding\"), list):\n",
        "                raise ValueError(f\"Invalid 'embedding' type: {type(record.get('embedding'))}\")\n",
        "            # Check metadata fields\n",
        "            for key, value in record.items():\n",
        "                if key not in (\"_id\", \"text\", \"embedding\"):\n",
        "                    if not isinstance(value, (str, int, float, bool, list)):\n",
        "                        raise ValueError(f\"Invalid type for {key}: {type(value)}\")\n",
        "                    if isinstance(value, list) and not all(isinstance(item, str) for item in value):\n",
        "                        raise ValueError(f\"Invalid list type for {key}: {value}\")\n",
        "\n",
        "        batch_size = 50\n",
        "        max_records_per_minute = token_limit_per_minute // tokens_per_record\n",
        "        records_per_second = max_records_per_minute / 60\n",
        "        seconds_per_batch = batch_size / records_per_second if records_per_second > 0 else 1\n",
        "\n",
        "        print(f\"Rate-limiting: ~{tokens_per_record} tokens/record, {token_limit_per_minute} tokens/minute, \"\n",
        "              f\"{max_records_per_minute} records/minute, {seconds_per_batch:.2f} seconds/batch\")\n",
        "\n",
        "        total_tokens = 0\n",
        "        for i in tqdm(range(0, len(records), batch_size), desc=\"Upserting to Pinecone\"):\n",
        "            batch = records[i:i + batch_size]\n",
        "            batch_tokens = len(batch) * tokens_per_record\n",
        "            total_tokens += batch_tokens\n",
        "\n",
        "            # Prepare Pinecone upsert format: (id, vector, metadata)\n",
        "            upsert_vectors = [\n",
        "                {\n",
        "                    \"id\": record[\"_id\"],\n",
        "                    \"values\": record[\"embedding\"],\n",
        "                    \"metadata\": {k: v for k, v in record.items() if k not in (\"_id\", \"embedding\")}\n",
        "                }\n",
        "                for record in batch\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                index.upsert(vectors=upsert_vectors, namespace=namespace)\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Upserted batch {i // batch_size + 1} with {len(batch)} records \"\n",
        "                      f\"(~{batch_tokens} tokens, total ~{total_tokens} tokens, took {elapsed_time:.2f}s)\")\n",
        "\n",
        "                if i + batch_size < len(records):\n",
        "                    sleep_time = max(0, seconds_per_batch - elapsed_time)\n",
        "                    if sleep_time > 0:\n",
        "                        print(f\"Sleeping for {sleep_time:.2f} seconds to respect rate limit\")\n",
        "                        time.sleep(sleep_time)\n",
        "            except Exception as batch_error:\n",
        "                print(f\"Error upserting batch {i // batch_size + 1}: {batch_error}\")\n",
        "                for record in batch[:5]:\n",
        "                    print(f\"Problematic record: {record}\")\n",
        "                raise\n",
        "        print(f\"Successfully upserted {len(records)} records into Pinecone index\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error upserting to Pinecone: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "dbG9Vl-SGG6a"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Orchestrate the Full Upsert Pipeline**"
      ],
      "metadata": {
        "id": "lN023oQf_BUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def digest_insert():\n",
        "    try:\n",
        "        # Download HTML files to GCS bucket\n",
        "        # download_html_to_gcs()\n",
        "\n",
        "        # Read JSON files from GCS\n",
        "        documents = read_json_from_gcs(BUCKET_NAME, GCS_PREFIX)\n",
        "\n",
        "        # Prepare records for upsert\n",
        "        records = prepare_records(documents)\n",
        "\n",
        "        # Embed document chunks\n",
        "        embedded_records = embed_documents(\n",
        "            records,\n",
        "            project_id=PROJECT_ID,\n",
        "            location=LOCATION,\n",
        "            model_name=EMBEDDING_MODEL,\n",
        "            fallback_model=FALLBACK_EMBEDDING_MODEL\n",
        "        )\n",
        "\n",
        "        # Upsert records to Pinecone\n",
        "        upsert_to_pinecone(embedded_records, NAMESPACE, tokens_per_record=250)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "# digest_insert()"
      ],
      "metadata": {
        "id": "kvAcqJhbGXs4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d3e1e77bae044febacb5ac2902ecaa0a",
            "587ce333422045e493ac4b35f307bb17",
            "3ad831465fd948c996782bb95ee5b968",
            "114c68fd1c9f4db6a0099be27066a0d5",
            "60ded670a3ae46bd8c219c5907230ff7",
            "e5fc7d9b3d1846d2b277b3280db9b3a1",
            "eff62d6a96ba42c687e4eae127551bfb",
            "eb9af13398624f0da69835f4e2296ea6",
            "f50862389bc345d690746bc3a9d4f4be",
            "3f710c8e6de749df905f802d27d2e745",
            "db99018587d24f4c8fe567f13cf26c32",
            "abefd6f7afc34dba84969264bad6679b",
            "d056c9ef2fb845679de3b0e2e7a19228",
            "1a846292529c4024940ed44212445ea1",
            "ebe1686c0cd24deaa4769988953e846b",
            "dd165800706b490a95714b01fc607473",
            "a01302cabafa465e96c1c8ff2eb1b66a",
            "83890de4904f48f4b912708c750d1397",
            "144fb548af6a4b01a79e632c299210c9",
            "58c4d99056f344289e90ad3c99469c30",
            "b3fcc27b03bb4cc185800da3da7d1269",
            "1e28aff6c3194753acd40baa2f6d70ee",
            "d4113517713946959da1c81c298f1949",
            "307b268bb53241a296bc5ab4c0ded838",
            "1593ec9ef9914444a9b0b2097243f218",
            "f519e1a9450e4293b4585878e4e4d0a4",
            "b21c4ea1d28742b8866bf20603c6644c",
            "82574c6728f94f928d6ee7d610f6be85",
            "b0fd78d1cff14d92804e8b1833a7bd86",
            "56ef245051a7482f96cba506f182bd58",
            "c7f9372579f94de79027dedcc13c3150",
            "b66c77f38ca343799777129b76be97f7",
            "7de76f7506de484498c54ea8b10dc0e0",
            "55110c6e01cc4df59fb4d155e3a07f4d",
            "4d24a484e207446493f8b1cba026c6a8",
            "b585c9ecd0c44ffcbaff23705fc56e18",
            "f7363f1d74de4176ac9a0912b79ac507",
            "753b55ae6c1a4a6381df7378958c8307",
            "c4505f47117f4799b66f0735636015ef",
            "181351224ee244abb05ce9d116b80efe",
            "a2a3e8f7df8a48c180d6cb162c6426d7",
            "cd526c24f56747d391efdd50c4d26f4b",
            "423e10d69fd64c47b0dc9886f0d67a1c",
            "e71c5e015277472c98733a1de2ff5d4f"
          ]
        },
        "outputId": "4141021f-4edf-402f-cc4f-54f7be8184c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3e1e77bae044febacb5ac2902ecaa0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Reading GCS files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file: documents_20250620200924.json\n",
            "Retrieved 101 documents from GCS\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abefd6f7afc34dba84969264bad6679b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Preparing records:   0%|          | 0/101 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunked text into 1 chunks\n",
            "Chunked text into 2 chunks\n",
            "Chunked text into 95 chunks\n",
            "Chunked text into 108 chunks\n",
            "Chunked text into 57 chunks\n",
            "Chunked text into 211 chunks\n",
            "Chunked text into 503 chunks\n",
            "Chunked text into 56 chunks\n",
            "Chunked text into 61 chunks\n",
            "Chunked text into 44 chunks\n",
            "Chunked text into 115 chunks\n",
            "Chunked text into 35 chunks\n",
            "Chunked text into 110 chunks\n",
            "Chunked text into 34 chunks\n",
            "Chunked text into 52 chunks\n",
            "Chunked text into 19 chunks\n",
            "Chunked text into 13 chunks\n",
            "Chunked text into 57 chunks\n",
            "Chunked text into 74 chunks\n",
            "Chunked text into 241 chunks\n",
            "Chunked text into 138 chunks\n",
            "Chunked text into 11 chunks\n",
            "Chunked text into 101 chunks\n",
            "Chunked text into 169 chunks\n",
            "Chunked text into 42 chunks\n",
            "Chunked text into 541 chunks\n",
            "Chunked text into 308 chunks\n",
            "Chunked text into 743 chunks\n",
            "Chunked text into 462 chunks\n",
            "Chunked text into 305 chunks\n",
            "Chunked text into 287 chunks\n",
            "Chunked text into 231 chunks\n",
            "Chunked text into 59 chunks\n",
            "Chunked text into 295 chunks\n",
            "Chunked text into 261 chunks\n",
            "Chunked text into 135 chunks\n",
            "Chunked text into 62 chunks\n",
            "Chunked text into 515 chunks\n",
            "Chunked text into 761 chunks\n",
            "Chunked text into 587 chunks\n",
            "Chunked text into 478 chunks\n",
            "Chunked text into 272 chunks\n",
            "Chunked text into 121 chunks\n",
            "Chunked text into 60 chunks\n",
            "Chunked text into 111 chunks\n",
            "Chunked text into 213 chunks\n",
            "Chunked text into 200 chunks\n",
            "Chunked text into 45 chunks\n",
            "Chunked text into 28 chunks\n",
            "Chunked text into 57 chunks\n",
            "Chunked text into 74 chunks\n",
            "Chunked text into 144 chunks\n",
            "Chunked text into 348 chunks\n",
            "Chunked text into 106 chunks\n",
            "Chunked text into 167 chunks\n",
            "Chunked text into 99 chunks\n",
            "Chunked text into 100 chunks\n",
            "Chunked text into 192 chunks\n",
            "Chunked text into 201 chunks\n",
            "Chunked text into 162 chunks\n",
            "Chunked text into 64 chunks\n",
            "Chunked text into 161 chunks\n",
            "Chunked text into 282 chunks\n",
            "Chunked text into 18 chunks\n",
            "Chunked text into 50 chunks\n",
            "Chunked text into 89 chunks\n",
            "Chunked text into 138 chunks\n",
            "Chunked text into 77 chunks\n",
            "Chunked text into 136 chunks\n",
            "Chunked text into 32 chunks\n",
            "Chunked text into 81 chunks\n",
            "Chunked text into 20 chunks\n",
            "Chunked text into 77 chunks\n",
            "Chunked text into 74 chunks\n",
            "Chunked text into 61 chunks\n",
            "Chunked text into 75 chunks\n",
            "Chunked text into 75 chunks\n",
            "Chunked text into 309 chunks\n",
            "Chunked text into 111 chunks\n",
            "Chunked text into 23 chunks\n",
            "Chunked text into 67 chunks\n",
            "Chunked text into 67 chunks\n",
            "Chunked text into 132 chunks\n",
            "Chunked text into 123 chunks\n",
            "Chunked text into 241 chunks\n",
            "Chunked text into 33 chunks\n",
            "Chunked text into 34 chunks\n",
            "Chunked text into 55 chunks\n",
            "Chunked text into 39 chunks\n",
            "Chunked text into 33 chunks\n",
            "Chunked text into 62 chunks\n",
            "Chunked text into 57 chunks\n",
            "Chunked text into 56 chunks\n",
            "Chunked text into 73 chunks\n",
            "Chunked text into 56 chunks\n",
            "Chunked text into 52 chunks\n",
            "Chunked text into 32 chunks\n",
            "Chunked text into 93 chunks\n",
            "Chunked text into 145 chunks\n",
            "Chunked text into 360 chunks\n",
            "Chunked text into 148 chunks\n",
            "Sample record: {'_id': 'c5d96f3a-a3c1-4963-82f9-7dbe5de006f8', 'text': 'IRS Chatbot Search', 'category': 'irs_info', 'title': 'IRS Chatbot Search', 'chunk_index': 0}\n",
            "Prepared 14990 valid records for upsert (skipped -42 invalid chunks)\n",
            "Using embedding model: text-embedding-005\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4113517713946959da1c81c298f1949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Embedding documents with text-embedding-005:   0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded batch 1 with 50 documents in 0.74s\n",
            "Embedded batch 2 with 50 documents in 0.54s\n",
            "Embedded batch 3 with 50 documents in 0.55s\n",
            "Embedded batch 4 with 50 documents in 0.59s\n",
            "Embedded batch 5 with 50 documents in 0.55s\n",
            "Embedded batch 6 with 50 documents in 0.51s\n",
            "Embedded batch 7 with 50 documents in 0.52s\n",
            "Embedded batch 8 with 50 documents in 0.71s\n",
            "Embedded batch 9 with 50 documents in 0.54s\n",
            "Embedded batch 10 with 50 documents in 0.52s\n",
            "Embedded batch 11 with 50 documents in 0.48s\n",
            "Embedded batch 12 with 50 documents in 0.62s\n",
            "Embedded batch 13 with 50 documents in 0.53s\n",
            "Embedded batch 14 with 50 documents in 0.52s\n",
            "Embedded batch 15 with 50 documents in 0.50s\n",
            "Embedded batch 16 with 50 documents in 0.53s\n",
            "Embedded batch 17 with 50 documents in 0.56s\n",
            "Embedded batch 18 with 50 documents in 0.51s\n",
            "Embedded batch 19 with 50 documents in 1.06s\n",
            "Embedded batch 20 with 50 documents in 0.51s\n",
            "Embedded batch 21 with 50 documents in 0.51s\n",
            "Embedded batch 22 with 50 documents in 0.54s\n",
            "Embedded batch 23 with 50 documents in 0.52s\n",
            "Embedded batch 24 with 50 documents in 0.55s\n",
            "Embedded batch 25 with 50 documents in 0.38s\n",
            "Embedded batch 26 with 50 documents in 0.38s\n",
            "Embedded batch 27 with 50 documents in 0.45s\n",
            "Embedded batch 28 with 50 documents in 0.44s\n",
            "Embedded batch 29 with 50 documents in 0.41s\n",
            "Embedded batch 30 with 50 documents in 0.39s\n",
            "Embedded batch 31 with 50 documents in 0.38s\n",
            "Embedded batch 32 with 50 documents in 0.38s\n",
            "Embedded batch 33 with 50 documents in 0.39s\n",
            "Embedded batch 34 with 50 documents in 0.41s\n",
            "Embedded batch 35 with 50 documents in 0.39s\n",
            "Embedded batch 36 with 50 documents in 0.50s\n",
            "Embedded batch 37 with 50 documents in 0.39s\n",
            "Embedded batch 38 with 50 documents in 0.56s\n",
            "Embedded batch 39 with 50 documents in 0.39s\n",
            "Embedded batch 40 with 50 documents in 0.91s\n",
            "Embedded batch 41 with 50 documents in 0.62s\n",
            "Embedded batch 42 with 50 documents in 0.39s\n",
            "Embedded batch 43 with 50 documents in 0.51s\n",
            "Embedded batch 44 with 50 documents in 0.48s\n",
            "Embedded batch 45 with 50 documents in 0.36s\n",
            "Embedded batch 46 with 50 documents in 0.38s\n",
            "Embedded batch 47 with 50 documents in 0.40s\n",
            "Embedded batch 48 with 50 documents in 0.41s\n",
            "Embedded batch 49 with 50 documents in 0.39s\n",
            "Embedded batch 50 with 50 documents in 0.41s\n",
            "Embedded batch 51 with 50 documents in 0.40s\n",
            "Embedded batch 52 with 50 documents in 0.44s\n",
            "Embedded batch 53 with 50 documents in 0.41s\n",
            "Embedded batch 54 with 50 documents in 0.37s\n",
            "Embedded batch 55 with 50 documents in 0.40s\n",
            "Embedded batch 56 with 50 documents in 0.39s\n",
            "Embedded batch 57 with 50 documents in 0.40s\n",
            "Embedded batch 58 with 50 documents in 0.39s\n",
            "Embedded batch 59 with 50 documents in 0.38s\n",
            "Embedded batch 60 with 50 documents in 0.47s\n",
            "Embedded batch 61 with 50 documents in 0.47s\n",
            "Embedded batch 62 with 50 documents in 0.42s\n",
            "Embedded batch 63 with 50 documents in 0.45s\n",
            "Embedded batch 64 with 50 documents in 0.47s\n",
            "Embedded batch 65 with 50 documents in 0.40s\n",
            "Embedded batch 66 with 50 documents in 0.72s\n",
            "Embedded batch 67 with 50 documents in 0.42s\n",
            "Embedded batch 68 with 50 documents in 0.46s\n",
            "Embedded batch 69 with 50 documents in 0.37s\n",
            "Embedded batch 70 with 50 documents in 0.42s\n",
            "Embedded batch 71 with 50 documents in 0.40s\n",
            "Embedded batch 72 with 50 documents in 0.38s\n",
            "Embedded batch 73 with 50 documents in 0.40s\n",
            "Embedded batch 74 with 50 documents in 0.42s\n",
            "Embedded batch 75 with 50 documents in 0.46s\n",
            "Embedded batch 76 with 50 documents in 0.57s\n",
            "Embedded batch 77 with 50 documents in 0.52s\n",
            "Embedded batch 78 with 50 documents in 0.41s\n",
            "Embedded batch 79 with 50 documents in 0.44s\n",
            "Embedded batch 80 with 50 documents in 0.37s\n",
            "Embedded batch 81 with 50 documents in 0.38s\n",
            "Embedded batch 82 with 50 documents in 0.38s\n",
            "Embedded batch 83 with 50 documents in 0.44s\n",
            "Embedded batch 84 with 50 documents in 0.38s\n",
            "Embedded batch 85 with 50 documents in 0.48s\n",
            "Embedded batch 86 with 50 documents in 0.38s\n",
            "Embedded batch 87 with 50 documents in 0.37s\n",
            "Embedded batch 88 with 50 documents in 0.36s\n",
            "Embedded batch 89 with 50 documents in 0.38s\n",
            "Embedded batch 90 with 50 documents in 0.38s\n",
            "Embedded batch 91 with 50 documents in 0.36s\n",
            "Embedded batch 92 with 50 documents in 0.39s\n",
            "Embedded batch 93 with 50 documents in 0.50s\n",
            "Embedded batch 94 with 50 documents in 0.38s\n",
            "Embedded batch 95 with 50 documents in 0.41s\n",
            "Embedded batch 96 with 50 documents in 0.41s\n",
            "Embedded batch 97 with 50 documents in 0.39s\n",
            "Embedded batch 98 with 50 documents in 0.37s\n",
            "Embedded batch 99 with 50 documents in 0.39s\n",
            "Embedded batch 100 with 50 documents in 0.46s\n",
            "Embedded batch 101 with 50 documents in 0.38s\n",
            "Embedded batch 102 with 50 documents in 0.62s\n",
            "Embedded batch 103 with 50 documents in 0.37s\n",
            "Embedded batch 104 with 50 documents in 0.40s\n",
            "Embedded batch 105 with 50 documents in 0.35s\n",
            "Embedded batch 106 with 50 documents in 0.36s\n",
            "Embedded batch 107 with 50 documents in 0.37s\n",
            "Embedded batch 108 with 50 documents in 0.39s\n",
            "Embedded batch 109 with 50 documents in 0.36s\n",
            "Embedded batch 110 with 50 documents in 0.46s\n",
            "Embedded batch 111 with 50 documents in 0.37s\n",
            "Embedded batch 112 with 50 documents in 0.40s\n",
            "Embedded batch 113 with 50 documents in 0.38s\n",
            "Embedded batch 114 with 50 documents in 0.38s\n",
            "Embedded batch 115 with 50 documents in 0.39s\n",
            "Embedded batch 116 with 50 documents in 0.39s\n",
            "Embedded batch 117 with 50 documents in 0.42s\n",
            "Embedded batch 118 with 50 documents in 0.49s\n",
            "Embedded batch 119 with 50 documents in 0.45s\n",
            "Embedded batch 120 with 50 documents in 0.72s\n",
            "Embedded batch 121 with 50 documents in 0.55s\n",
            "Embedded batch 122 with 50 documents in 0.49s\n",
            "Embedded batch 123 with 50 documents in 0.45s\n",
            "Embedded batch 124 with 50 documents in 0.39s\n",
            "Embedded batch 125 with 50 documents in 0.40s\n",
            "Embedded batch 126 with 50 documents in 0.49s\n",
            "Embedded batch 127 with 50 documents in 0.46s\n",
            "Embedded batch 128 with 50 documents in 0.41s\n",
            "Embedded batch 129 with 50 documents in 0.36s\n",
            "Embedded batch 130 with 50 documents in 0.45s\n",
            "Embedded batch 131 with 50 documents in 0.43s\n",
            "Embedded batch 132 with 50 documents in 0.46s\n",
            "Embedded batch 133 with 50 documents in 0.39s\n",
            "Embedded batch 134 with 50 documents in 0.45s\n",
            "Embedded batch 135 with 50 documents in 0.45s\n",
            "Embedded batch 136 with 50 documents in 0.43s\n",
            "Embedded batch 137 with 50 documents in 0.39s\n",
            "Embedded batch 138 with 50 documents in 0.54s\n",
            "Embedded batch 139 with 50 documents in 0.38s\n",
            "Embedded batch 140 with 50 documents in 0.39s\n",
            "Embedded batch 141 with 50 documents in 0.43s\n",
            "Embedded batch 142 with 50 documents in 0.42s\n",
            "Embedded batch 143 with 50 documents in 0.47s\n",
            "Embedded batch 144 with 50 documents in 0.44s\n",
            "Embedded batch 145 with 50 documents in 0.41s\n",
            "Embedded batch 146 with 50 documents in 0.50s\n",
            "Embedded batch 147 with 50 documents in 0.38s\n",
            "Embedded batch 148 with 50 documents in 0.90s\n",
            "Embedded batch 149 with 50 documents in 0.42s\n",
            "Embedded batch 150 with 50 documents in 0.50s\n",
            "Embedded batch 151 with 50 documents in 0.52s\n",
            "Embedded batch 152 with 50 documents in 0.38s\n",
            "Embedded batch 153 with 50 documents in 0.45s\n",
            "Embedded batch 154 with 50 documents in 0.43s\n",
            "Embedded batch 155 with 50 documents in 0.46s\n",
            "Embedded batch 156 with 50 documents in 0.47s\n",
            "Embedded batch 157 with 50 documents in 0.46s\n",
            "Embedded batch 158 with 50 documents in 0.39s\n",
            "Embedded batch 159 with 50 documents in 0.56s\n",
            "Embedded batch 160 with 50 documents in 0.40s\n",
            "Embedded batch 161 with 50 documents in 0.39s\n",
            "Embedded batch 162 with 50 documents in 0.36s\n",
            "Embedded batch 163 with 50 documents in 0.41s\n",
            "Embedded batch 164 with 50 documents in 0.35s\n",
            "Embedded batch 165 with 50 documents in 0.37s\n",
            "Embedded batch 166 with 50 documents in 0.38s\n",
            "Embedded batch 167 with 50 documents in 0.52s\n",
            "Embedded batch 168 with 50 documents in 0.37s\n",
            "Embedded batch 169 with 50 documents in 0.45s\n",
            "Embedded batch 170 with 50 documents in 0.39s\n",
            "Embedded batch 171 with 50 documents in 0.37s\n",
            "Embedded batch 172 with 50 documents in 0.37s\n",
            "Embedded batch 173 with 50 documents in 0.38s\n",
            "Embedded batch 174 with 50 documents in 0.37s\n",
            "Embedded batch 175 with 50 documents in 0.45s\n",
            "Embedded batch 176 with 50 documents in 0.49s\n",
            "Embedded batch 177 with 50 documents in 0.67s\n",
            "Embedded batch 178 with 50 documents in 0.54s\n",
            "Embedded batch 179 with 50 documents in 0.53s\n",
            "Embedded batch 180 with 50 documents in 0.58s\n",
            "Embedded batch 181 with 50 documents in 0.52s\n",
            "Embedded batch 182 with 50 documents in 0.55s\n",
            "Embedded batch 183 with 50 documents in 0.64s\n",
            "Embedded batch 184 with 50 documents in 0.52s\n",
            "Embedded batch 185 with 50 documents in 0.55s\n",
            "Embedded batch 186 with 50 documents in 0.55s\n",
            "Embedded batch 187 with 50 documents in 0.53s\n",
            "Embedded batch 188 with 50 documents in 0.53s\n",
            "Embedded batch 189 with 50 documents in 0.53s\n",
            "Embedded batch 190 with 50 documents in 0.53s\n",
            "Embedded batch 191 with 50 documents in 0.61s\n",
            "Embedded batch 192 with 50 documents in 0.51s\n",
            "Embedded batch 193 with 50 documents in 0.53s\n",
            "Embedded batch 194 with 50 documents in 0.53s\n",
            "Embedded batch 195 with 50 documents in 0.52s\n",
            "Embedded batch 196 with 50 documents in 0.54s\n",
            "Embedded batch 197 with 50 documents in 0.54s\n",
            "Embedded batch 198 with 50 documents in 0.59s\n",
            "Embedded batch 199 with 50 documents in 0.55s\n",
            "Embedded batch 200 with 50 documents in 0.49s\n",
            "Embedded batch 201 with 50 documents in 0.50s\n",
            "Embedded batch 202 with 50 documents in 0.53s\n",
            "Embedded batch 203 with 50 documents in 0.51s\n",
            "Embedded batch 204 with 50 documents in 0.51s\n",
            "Embedded batch 205 with 50 documents in 0.39s\n",
            "Embedded batch 206 with 50 documents in 0.57s\n",
            "Embedded batch 207 with 50 documents in 0.89s\n",
            "Embedded batch 208 with 50 documents in 0.43s\n",
            "Embedded batch 209 with 50 documents in 0.42s\n",
            "Embedded batch 210 with 50 documents in 0.43s\n",
            "Embedded batch 211 with 50 documents in 0.47s\n",
            "Embedded batch 212 with 50 documents in 0.45s\n",
            "Embedded batch 213 with 50 documents in 0.43s\n",
            "Embedded batch 214 with 50 documents in 0.62s\n",
            "Embedded batch 215 with 50 documents in 0.40s\n",
            "Embedded batch 216 with 50 documents in 0.58s\n",
            "Embedded batch 217 with 50 documents in 0.40s\n",
            "Embedded batch 218 with 50 documents in 0.45s\n",
            "Embedded batch 219 with 50 documents in 0.46s\n",
            "Embedded batch 220 with 50 documents in 0.89s\n",
            "Embedded batch 221 with 50 documents in 0.43s\n",
            "Embedded batch 222 with 50 documents in 0.53s\n",
            "Embedded batch 223 with 50 documents in 0.41s\n",
            "Embedded batch 224 with 50 documents in 0.47s\n",
            "Embedded batch 225 with 50 documents in 0.44s\n",
            "Embedded batch 226 with 50 documents in 0.46s\n",
            "Embedded batch 227 with 50 documents in 0.44s\n",
            "Embedded batch 228 with 50 documents in 0.43s\n",
            "Embedded batch 229 with 50 documents in 0.42s\n",
            "Embedded batch 230 with 50 documents in 0.52s\n",
            "Embedded batch 231 with 50 documents in 0.61s\n",
            "Embedded batch 232 with 50 documents in 0.44s\n",
            "Embedded batch 233 with 50 documents in 0.39s\n",
            "Embedded batch 234 with 50 documents in 0.39s\n",
            "Embedded batch 235 with 50 documents in 0.77s\n",
            "Embedded batch 236 with 50 documents in 0.54s\n",
            "Embedded batch 237 with 50 documents in 0.48s\n",
            "Embedded batch 238 with 50 documents in 0.84s\n",
            "Embedded batch 239 with 50 documents in 0.42s\n",
            "Embedded batch 240 with 50 documents in 0.55s\n",
            "Embedded batch 241 with 50 documents in 0.50s\n",
            "Embedded batch 242 with 50 documents in 0.42s\n",
            "Embedded batch 243 with 50 documents in 0.56s\n",
            "Embedded batch 244 with 50 documents in 0.44s\n",
            "Embedded batch 245 with 50 documents in 0.66s\n",
            "Embedded batch 246 with 50 documents in 0.43s\n",
            "Embedded batch 247 with 50 documents in 0.41s\n",
            "Embedded batch 248 with 50 documents in 0.41s\n",
            "Embedded batch 249 with 50 documents in 0.46s\n",
            "Embedded batch 250 with 50 documents in 0.58s\n",
            "Embedded batch 251 with 50 documents in 0.42s\n",
            "Embedded batch 252 with 50 documents in 0.45s\n",
            "Embedded batch 253 with 50 documents in 0.63s\n",
            "Embedded batch 254 with 50 documents in 0.42s\n",
            "Embedded batch 255 with 50 documents in 0.40s\n",
            "Embedded batch 256 with 50 documents in 0.40s\n",
            "Embedded batch 257 with 50 documents in 0.42s\n",
            "Embedded batch 258 with 50 documents in 0.40s\n",
            "Embedded batch 259 with 50 documents in 0.41s\n",
            "Embedded batch 260 with 50 documents in 0.37s\n",
            "Embedded batch 261 with 50 documents in 0.49s\n",
            "Embedded batch 262 with 50 documents in 0.41s\n",
            "Embedded batch 263 with 50 documents in 0.41s\n",
            "Embedded batch 264 with 50 documents in 0.42s\n",
            "Embedded batch 265 with 50 documents in 0.38s\n",
            "Embedded batch 266 with 50 documents in 0.40s\n",
            "Embedded batch 267 with 50 documents in 0.43s\n",
            "Embedded batch 268 with 50 documents in 0.44s\n",
            "Embedded batch 269 with 50 documents in 0.53s\n",
            "Embedded batch 270 with 50 documents in 0.40s\n",
            "Embedded batch 271 with 50 documents in 0.39s\n",
            "Embedded batch 272 with 50 documents in 0.56s\n",
            "Embedded batch 273 with 50 documents in 0.41s\n",
            "Embedded batch 274 with 50 documents in 0.52s\n",
            "Embedded batch 275 with 50 documents in 0.41s\n",
            "Embedded batch 276 with 50 documents in 0.45s\n",
            "Embedded batch 277 with 50 documents in 0.49s\n",
            "Embedded batch 278 with 50 documents in 0.44s\n",
            "Embedded batch 279 with 50 documents in 0.41s\n",
            "Embedded batch 280 with 50 documents in 0.41s\n",
            "Embedded batch 281 with 50 documents in 0.40s\n",
            "Embedded batch 282 with 50 documents in 0.42s\n",
            "Embedded batch 283 with 50 documents in 0.41s\n",
            "Embedded batch 284 with 50 documents in 0.43s\n",
            "Embedded batch 285 with 50 documents in 0.45s\n",
            "Embedded batch 286 with 50 documents in 0.42s\n",
            "Embedded batch 287 with 50 documents in 0.44s\n",
            "Embedded batch 288 with 50 documents in 0.42s\n",
            "Embedded batch 289 with 50 documents in 0.43s\n",
            "Embedded batch 290 with 50 documents in 0.42s\n",
            "Embedded batch 291 with 50 documents in 0.41s\n",
            "Embedded batch 292 with 50 documents in 0.39s\n",
            "Embedded batch 293 with 50 documents in 0.52s\n",
            "Embedded batch 294 with 50 documents in 0.43s\n",
            "Embedded batch 295 with 50 documents in 0.38s\n",
            "Embedded batch 296 with 50 documents in 0.40s\n",
            "Embedded batch 297 with 50 documents in 0.43s\n",
            "Embedded batch 298 with 50 documents in 0.40s\n",
            "Embedded batch 299 with 50 documents in 0.42s\n",
            "Embedded batch 300 with 40 documents in 0.32s\n",
            "Successfully embedded 14990 documents with text-embedding-005\n",
            "Rate-limiting: ~250 tokens/record, 250000 tokens/minute, 1000 records/minute, 3.00 seconds/batch\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55110c6e01cc4df59fb4d155e3a07f4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upserting to Pinecone:   0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted batch 1 with 50 records (~12500 tokens, total ~12500 tokens, took 1.04s)\n",
            "Sleeping for 1.96 seconds to respect rate limit\n",
            "Upserted batch 2 with 50 records (~12500 tokens, total ~25000 tokens, took 0.54s)\n",
            "Sleeping for 2.46 seconds to respect rate limit\n",
            "Upserted batch 3 with 50 records (~12500 tokens, total ~37500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 4 with 50 records (~12500 tokens, total ~50000 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 5 with 50 records (~12500 tokens, total ~62500 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 6 with 50 records (~12500 tokens, total ~75000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 7 with 50 records (~12500 tokens, total ~87500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 8 with 50 records (~12500 tokens, total ~100000 tokens, took 0.77s)\n",
            "Sleeping for 2.23 seconds to respect rate limit\n",
            "Upserted batch 9 with 50 records (~12500 tokens, total ~112500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 10 with 50 records (~12500 tokens, total ~125000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 11 with 50 records (~12500 tokens, total ~137500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 12 with 50 records (~12500 tokens, total ~150000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 13 with 50 records (~12500 tokens, total ~162500 tokens, took 0.79s)\n",
            "Sleeping for 2.21 seconds to respect rate limit\n",
            "Upserted batch 14 with 50 records (~12500 tokens, total ~175000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 15 with 50 records (~12500 tokens, total ~187500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 16 with 50 records (~12500 tokens, total ~200000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 17 with 50 records (~12500 tokens, total ~212500 tokens, took 0.77s)\n",
            "Sleeping for 2.23 seconds to respect rate limit\n",
            "Upserted batch 18 with 50 records (~12500 tokens, total ~225000 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 19 with 50 records (~12500 tokens, total ~237500 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 20 with 50 records (~12500 tokens, total ~250000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 21 with 50 records (~12500 tokens, total ~262500 tokens, took 0.85s)\n",
            "Sleeping for 2.15 seconds to respect rate limit\n",
            "Upserted batch 22 with 50 records (~12500 tokens, total ~275000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 23 with 50 records (~12500 tokens, total ~287500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 24 with 50 records (~12500 tokens, total ~300000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 25 with 50 records (~12500 tokens, total ~312500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 26 with 50 records (~12500 tokens, total ~325000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 27 with 50 records (~12500 tokens, total ~337500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 28 with 50 records (~12500 tokens, total ~350000 tokens, took 0.71s)\n",
            "Sleeping for 2.29 seconds to respect rate limit\n",
            "Upserted batch 29 with 50 records (~12500 tokens, total ~362500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 30 with 50 records (~12500 tokens, total ~375000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 31 with 50 records (~12500 tokens, total ~387500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 32 with 50 records (~12500 tokens, total ~400000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 33 with 50 records (~12500 tokens, total ~412500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 34 with 50 records (~12500 tokens, total ~425000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 35 with 50 records (~12500 tokens, total ~437500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 36 with 50 records (~12500 tokens, total ~450000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 37 with 50 records (~12500 tokens, total ~462500 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 38 with 50 records (~12500 tokens, total ~475000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 39 with 50 records (~12500 tokens, total ~487500 tokens, took 0.69s)\n",
            "Sleeping for 2.31 seconds to respect rate limit\n",
            "Upserted batch 40 with 50 records (~12500 tokens, total ~500000 tokens, took 0.79s)\n",
            "Sleeping for 2.21 seconds to respect rate limit\n",
            "Upserted batch 41 with 50 records (~12500 tokens, total ~512500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 42 with 50 records (~12500 tokens, total ~525000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 43 with 50 records (~12500 tokens, total ~537500 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 44 with 50 records (~12500 tokens, total ~550000 tokens, took 0.85s)\n",
            "Sleeping for 2.15 seconds to respect rate limit\n",
            "Upserted batch 45 with 50 records (~12500 tokens, total ~562500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 46 with 50 records (~12500 tokens, total ~575000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 47 with 50 records (~12500 tokens, total ~587500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 48 with 50 records (~12500 tokens, total ~600000 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 49 with 50 records (~12500 tokens, total ~612500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 50 with 50 records (~12500 tokens, total ~625000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 51 with 50 records (~12500 tokens, total ~637500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 52 with 50 records (~12500 tokens, total ~650000 tokens, took 0.76s)\n",
            "Sleeping for 2.24 seconds to respect rate limit\n",
            "Upserted batch 53 with 50 records (~12500 tokens, total ~662500 tokens, took 0.55s)\n",
            "Sleeping for 2.45 seconds to respect rate limit\n",
            "Upserted batch 54 with 50 records (~12500 tokens, total ~675000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 55 with 50 records (~12500 tokens, total ~687500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 56 with 50 records (~12500 tokens, total ~700000 tokens, took 0.85s)\n",
            "Sleeping for 2.15 seconds to respect rate limit\n",
            "Upserted batch 57 with 50 records (~12500 tokens, total ~712500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 58 with 50 records (~12500 tokens, total ~725000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 59 with 50 records (~12500 tokens, total ~737500 tokens, took 0.74s)\n",
            "Sleeping for 2.26 seconds to respect rate limit\n",
            "Upserted batch 60 with 50 records (~12500 tokens, total ~750000 tokens, took 0.83s)\n",
            "Sleeping for 2.17 seconds to respect rate limit\n",
            "Upserted batch 61 with 50 records (~12500 tokens, total ~762500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 62 with 50 records (~12500 tokens, total ~775000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 63 with 50 records (~12500 tokens, total ~787500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 64 with 50 records (~12500 tokens, total ~800000 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 65 with 50 records (~12500 tokens, total ~812500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 66 with 50 records (~12500 tokens, total ~825000 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 67 with 50 records (~12500 tokens, total ~837500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 68 with 50 records (~12500 tokens, total ~850000 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 69 with 50 records (~12500 tokens, total ~862500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 70 with 50 records (~12500 tokens, total ~875000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 71 with 50 records (~12500 tokens, total ~887500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 72 with 50 records (~12500 tokens, total ~900000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 73 with 50 records (~12500 tokens, total ~912500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 74 with 50 records (~12500 tokens, total ~925000 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 75 with 50 records (~12500 tokens, total ~937500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 76 with 50 records (~12500 tokens, total ~950000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 77 with 50 records (~12500 tokens, total ~962500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 78 with 50 records (~12500 tokens, total ~975000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 79 with 50 records (~12500 tokens, total ~987500 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 80 with 50 records (~12500 tokens, total ~1000000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 81 with 50 records (~12500 tokens, total ~1012500 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 82 with 50 records (~12500 tokens, total ~1025000 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 83 with 50 records (~12500 tokens, total ~1037500 tokens, took 0.86s)\n",
            "Sleeping for 2.14 seconds to respect rate limit\n",
            "Upserted batch 84 with 50 records (~12500 tokens, total ~1050000 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 85 with 50 records (~12500 tokens, total ~1062500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 86 with 50 records (~12500 tokens, total ~1075000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 87 with 50 records (~12500 tokens, total ~1087500 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 88 with 50 records (~12500 tokens, total ~1100000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 89 with 50 records (~12500 tokens, total ~1112500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 90 with 50 records (~12500 tokens, total ~1125000 tokens, took 0.74s)\n",
            "Sleeping for 2.26 seconds to respect rate limit\n",
            "Upserted batch 91 with 50 records (~12500 tokens, total ~1137500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 92 with 50 records (~12500 tokens, total ~1150000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 93 with 50 records (~12500 tokens, total ~1162500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 94 with 50 records (~12500 tokens, total ~1175000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 95 with 50 records (~12500 tokens, total ~1187500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 96 with 50 records (~12500 tokens, total ~1200000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 97 with 50 records (~12500 tokens, total ~1212500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 98 with 50 records (~12500 tokens, total ~1225000 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 99 with 50 records (~12500 tokens, total ~1237500 tokens, took 0.73s)\n",
            "Sleeping for 2.27 seconds to respect rate limit\n",
            "Upserted batch 100 with 50 records (~12500 tokens, total ~1250000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 101 with 50 records (~12500 tokens, total ~1262500 tokens, took 0.69s)\n",
            "Sleeping for 2.31 seconds to respect rate limit\n",
            "Upserted batch 102 with 50 records (~12500 tokens, total ~1275000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 103 with 50 records (~12500 tokens, total ~1287500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 104 with 50 records (~12500 tokens, total ~1300000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 105 with 50 records (~12500 tokens, total ~1312500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 106 with 50 records (~12500 tokens, total ~1325000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 107 with 50 records (~12500 tokens, total ~1337500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 108 with 50 records (~12500 tokens, total ~1350000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 109 with 50 records (~12500 tokens, total ~1362500 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 110 with 50 records (~12500 tokens, total ~1375000 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 111 with 50 records (~12500 tokens, total ~1387500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 112 with 50 records (~12500 tokens, total ~1400000 tokens, took 0.69s)\n",
            "Sleeping for 2.31 seconds to respect rate limit\n",
            "Upserted batch 113 with 50 records (~12500 tokens, total ~1412500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 114 with 50 records (~12500 tokens, total ~1425000 tokens, took 0.85s)\n",
            "Sleeping for 2.15 seconds to respect rate limit\n",
            "Upserted batch 115 with 50 records (~12500 tokens, total ~1437500 tokens, took 0.76s)\n",
            "Sleeping for 2.24 seconds to respect rate limit\n",
            "Upserted batch 116 with 50 records (~12500 tokens, total ~1450000 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 117 with 50 records (~12500 tokens, total ~1462500 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 118 with 50 records (~12500 tokens, total ~1475000 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 119 with 50 records (~12500 tokens, total ~1487500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 120 with 50 records (~12500 tokens, total ~1500000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 121 with 50 records (~12500 tokens, total ~1512500 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 122 with 50 records (~12500 tokens, total ~1525000 tokens, took 0.85s)\n",
            "Sleeping for 2.15 seconds to respect rate limit\n",
            "Upserted batch 123 with 50 records (~12500 tokens, total ~1537500 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 124 with 50 records (~12500 tokens, total ~1550000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 125 with 50 records (~12500 tokens, total ~1562500 tokens, took 0.53s)\n",
            "Sleeping for 2.47 seconds to respect rate limit\n",
            "Upserted batch 126 with 50 records (~12500 tokens, total ~1575000 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 127 with 50 records (~12500 tokens, total ~1587500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 128 with 50 records (~12500 tokens, total ~1600000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 129 with 50 records (~12500 tokens, total ~1612500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 130 with 50 records (~12500 tokens, total ~1625000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 131 with 50 records (~12500 tokens, total ~1637500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 132 with 50 records (~12500 tokens, total ~1650000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 133 with 50 records (~12500 tokens, total ~1662500 tokens, took 0.72s)\n",
            "Sleeping for 2.28 seconds to respect rate limit\n",
            "Upserted batch 134 with 50 records (~12500 tokens, total ~1675000 tokens, took 0.72s)\n",
            "Sleeping for 2.28 seconds to respect rate limit\n",
            "Upserted batch 135 with 50 records (~12500 tokens, total ~1687500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 136 with 50 records (~12500 tokens, total ~1700000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 137 with 50 records (~12500 tokens, total ~1712500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 138 with 50 records (~12500 tokens, total ~1725000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 139 with 50 records (~12500 tokens, total ~1737500 tokens, took 0.73s)\n",
            "Sleeping for 2.27 seconds to respect rate limit\n",
            "Upserted batch 140 with 50 records (~12500 tokens, total ~1750000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 141 with 50 records (~12500 tokens, total ~1762500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 142 with 50 records (~12500 tokens, total ~1775000 tokens, took 0.73s)\n",
            "Sleeping for 2.27 seconds to respect rate limit\n",
            "Upserted batch 143 with 50 records (~12500 tokens, total ~1787500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 144 with 50 records (~12500 tokens, total ~1800000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 145 with 50 records (~12500 tokens, total ~1812500 tokens, took 0.84s)\n",
            "Sleeping for 2.16 seconds to respect rate limit\n",
            "Upserted batch 146 with 50 records (~12500 tokens, total ~1825000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 147 with 50 records (~12500 tokens, total ~1837500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 148 with 50 records (~12500 tokens, total ~1850000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 149 with 50 records (~12500 tokens, total ~1862500 tokens, took 0.77s)\n",
            "Sleeping for 2.23 seconds to respect rate limit\n",
            "Upserted batch 150 with 50 records (~12500 tokens, total ~1875000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 151 with 50 records (~12500 tokens, total ~1887500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 152 with 50 records (~12500 tokens, total ~1900000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 153 with 50 records (~12500 tokens, total ~1912500 tokens, took 0.75s)\n",
            "Sleeping for 2.25 seconds to respect rate limit\n",
            "Upserted batch 154 with 50 records (~12500 tokens, total ~1925000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 155 with 50 records (~12500 tokens, total ~1937500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 156 with 50 records (~12500 tokens, total ~1950000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 157 with 50 records (~12500 tokens, total ~1962500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 158 with 50 records (~12500 tokens, total ~1975000 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 159 with 50 records (~12500 tokens, total ~1987500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 160 with 50 records (~12500 tokens, total ~2000000 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 161 with 50 records (~12500 tokens, total ~2012500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 162 with 50 records (~12500 tokens, total ~2025000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 163 with 50 records (~12500 tokens, total ~2037500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 164 with 50 records (~12500 tokens, total ~2050000 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 165 with 50 records (~12500 tokens, total ~2062500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 166 with 50 records (~12500 tokens, total ~2075000 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 167 with 50 records (~12500 tokens, total ~2087500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 168 with 50 records (~12500 tokens, total ~2100000 tokens, took 0.79s)\n",
            "Sleeping for 2.21 seconds to respect rate limit\n",
            "Upserted batch 169 with 50 records (~12500 tokens, total ~2112500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 170 with 50 records (~12500 tokens, total ~2125000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 171 with 50 records (~12500 tokens, total ~2137500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 172 with 50 records (~12500 tokens, total ~2150000 tokens, took 0.89s)\n",
            "Sleeping for 2.11 seconds to respect rate limit\n",
            "Upserted batch 173 with 50 records (~12500 tokens, total ~2162500 tokens, took 0.69s)\n",
            "Sleeping for 2.31 seconds to respect rate limit\n",
            "Upserted batch 174 with 50 records (~12500 tokens, total ~2175000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 175 with 50 records (~12500 tokens, total ~2187500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 176 with 50 records (~12500 tokens, total ~2200000 tokens, took 0.84s)\n",
            "Sleeping for 2.16 seconds to respect rate limit\n",
            "Upserted batch 177 with 50 records (~12500 tokens, total ~2212500 tokens, took 0.72s)\n",
            "Sleeping for 2.28 seconds to respect rate limit\n",
            "Upserted batch 178 with 50 records (~12500 tokens, total ~2225000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 179 with 50 records (~12500 tokens, total ~2237500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 180 with 50 records (~12500 tokens, total ~2250000 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 181 with 50 records (~12500 tokens, total ~2262500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 182 with 50 records (~12500 tokens, total ~2275000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 183 with 50 records (~12500 tokens, total ~2287500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 184 with 50 records (~12500 tokens, total ~2300000 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 185 with 50 records (~12500 tokens, total ~2312500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 186 with 50 records (~12500 tokens, total ~2325000 tokens, took 0.73s)\n",
            "Sleeping for 2.27 seconds to respect rate limit\n",
            "Upserted batch 187 with 50 records (~12500 tokens, total ~2337500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 188 with 50 records (~12500 tokens, total ~2350000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 189 with 50 records (~12500 tokens, total ~2362500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 190 with 50 records (~12500 tokens, total ~2375000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 191 with 50 records (~12500 tokens, total ~2387500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 192 with 50 records (~12500 tokens, total ~2400000 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 193 with 50 records (~12500 tokens, total ~2412500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 194 with 50 records (~12500 tokens, total ~2425000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 195 with 50 records (~12500 tokens, total ~2437500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 196 with 50 records (~12500 tokens, total ~2450000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 197 with 50 records (~12500 tokens, total ~2462500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 198 with 50 records (~12500 tokens, total ~2475000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 199 with 50 records (~12500 tokens, total ~2487500 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 200 with 50 records (~12500 tokens, total ~2500000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 201 with 50 records (~12500 tokens, total ~2512500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 202 with 50 records (~12500 tokens, total ~2525000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 203 with 50 records (~12500 tokens, total ~2537500 tokens, took 0.79s)\n",
            "Sleeping for 2.21 seconds to respect rate limit\n",
            "Upserted batch 204 with 50 records (~12500 tokens, total ~2550000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 205 with 50 records (~12500 tokens, total ~2562500 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 206 with 50 records (~12500 tokens, total ~2575000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 207 with 50 records (~12500 tokens, total ~2587500 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 208 with 50 records (~12500 tokens, total ~2600000 tokens, took 0.55s)\n",
            "Sleeping for 2.45 seconds to respect rate limit\n",
            "Upserted batch 209 with 50 records (~12500 tokens, total ~2612500 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 210 with 50 records (~12500 tokens, total ~2625000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 211 with 50 records (~12500 tokens, total ~2637500 tokens, took 0.83s)\n",
            "Sleeping for 2.17 seconds to respect rate limit\n",
            "Upserted batch 212 with 50 records (~12500 tokens, total ~2650000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 213 with 50 records (~12500 tokens, total ~2662500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 214 with 50 records (~12500 tokens, total ~2675000 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 215 with 50 records (~12500 tokens, total ~2687500 tokens, took 0.80s)\n",
            "Sleeping for 2.20 seconds to respect rate limit\n",
            "Upserted batch 216 with 50 records (~12500 tokens, total ~2700000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 217 with 50 records (~12500 tokens, total ~2712500 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 218 with 50 records (~12500 tokens, total ~2725000 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 219 with 50 records (~12500 tokens, total ~2737500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 220 with 50 records (~12500 tokens, total ~2750000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 221 with 50 records (~12500 tokens, total ~2762500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 222 with 50 records (~12500 tokens, total ~2775000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 223 with 50 records (~12500 tokens, total ~2787500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 224 with 50 records (~12500 tokens, total ~2800000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 225 with 50 records (~12500 tokens, total ~2812500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 226 with 50 records (~12500 tokens, total ~2825000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 227 with 50 records (~12500 tokens, total ~2837500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 228 with 50 records (~12500 tokens, total ~2850000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 229 with 50 records (~12500 tokens, total ~2862500 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 230 with 50 records (~12500 tokens, total ~2875000 tokens, took 0.89s)\n",
            "Sleeping for 2.11 seconds to respect rate limit\n",
            "Upserted batch 231 with 50 records (~12500 tokens, total ~2887500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 232 with 50 records (~12500 tokens, total ~2900000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 233 with 50 records (~12500 tokens, total ~2912500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 234 with 50 records (~12500 tokens, total ~2925000 tokens, took 0.93s)\n",
            "Sleeping for 2.07 seconds to respect rate limit\n",
            "Upserted batch 235 with 50 records (~12500 tokens, total ~2937500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 236 with 50 records (~12500 tokens, total ~2950000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 237 with 50 records (~12500 tokens, total ~2962500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 238 with 50 records (~12500 tokens, total ~2975000 tokens, took 0.82s)\n",
            "Sleeping for 2.18 seconds to respect rate limit\n",
            "Upserted batch 239 with 50 records (~12500 tokens, total ~2987500 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 240 with 50 records (~12500 tokens, total ~3000000 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 241 with 50 records (~12500 tokens, total ~3012500 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 242 with 50 records (~12500 tokens, total ~3025000 tokens, took 0.83s)\n",
            "Sleeping for 2.17 seconds to respect rate limit\n",
            "Upserted batch 243 with 50 records (~12500 tokens, total ~3037500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 244 with 50 records (~12500 tokens, total ~3050000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 245 with 50 records (~12500 tokens, total ~3062500 tokens, took 0.74s)\n",
            "Sleeping for 2.26 seconds to respect rate limit\n",
            "Upserted batch 246 with 50 records (~12500 tokens, total ~3075000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 247 with 50 records (~12500 tokens, total ~3087500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 248 with 50 records (~12500 tokens, total ~3100000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 249 with 50 records (~12500 tokens, total ~3112500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 250 with 50 records (~12500 tokens, total ~3125000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 251 with 50 records (~12500 tokens, total ~3137500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 252 with 50 records (~12500 tokens, total ~3150000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 253 with 50 records (~12500 tokens, total ~3162500 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 254 with 50 records (~12500 tokens, total ~3175000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 255 with 50 records (~12500 tokens, total ~3187500 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 256 with 50 records (~12500 tokens, total ~3200000 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 257 with 50 records (~12500 tokens, total ~3212500 tokens, took 0.73s)\n",
            "Sleeping for 2.27 seconds to respect rate limit\n",
            "Upserted batch 258 with 50 records (~12500 tokens, total ~3225000 tokens, took 0.74s)\n",
            "Sleeping for 2.26 seconds to respect rate limit\n",
            "Upserted batch 259 with 50 records (~12500 tokens, total ~3237500 tokens, took 0.64s)\n",
            "Sleeping for 2.36 seconds to respect rate limit\n",
            "Upserted batch 260 with 50 records (~12500 tokens, total ~3250000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 261 with 50 records (~12500 tokens, total ~3262500 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 262 with 50 records (~12500 tokens, total ~3275000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 263 with 50 records (~12500 tokens, total ~3287500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 264 with 50 records (~12500 tokens, total ~3300000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 265 with 50 records (~12500 tokens, total ~3312500 tokens, took 0.74s)\n",
            "Sleeping for 2.26 seconds to respect rate limit\n",
            "Upserted batch 266 with 50 records (~12500 tokens, total ~3325000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 267 with 50 records (~12500 tokens, total ~3337500 tokens, took 0.75s)\n",
            "Sleeping for 2.25 seconds to respect rate limit\n",
            "Upserted batch 268 with 50 records (~12500 tokens, total ~3350000 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 269 with 50 records (~12500 tokens, total ~3362500 tokens, took 0.81s)\n",
            "Sleeping for 2.19 seconds to respect rate limit\n",
            "Upserted batch 270 with 50 records (~12500 tokens, total ~3375000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 271 with 50 records (~12500 tokens, total ~3387500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 272 with 50 records (~12500 tokens, total ~3400000 tokens, took 0.69s)\n",
            "Sleeping for 2.31 seconds to respect rate limit\n",
            "Upserted batch 273 with 50 records (~12500 tokens, total ~3412500 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 274 with 50 records (~12500 tokens, total ~3425000 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 275 with 50 records (~12500 tokens, total ~3437500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 276 with 50 records (~12500 tokens, total ~3450000 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 277 with 50 records (~12500 tokens, total ~3462500 tokens, took 0.63s)\n",
            "Sleeping for 2.37 seconds to respect rate limit\n",
            "Upserted batch 278 with 50 records (~12500 tokens, total ~3475000 tokens, took 0.70s)\n",
            "Sleeping for 2.30 seconds to respect rate limit\n",
            "Upserted batch 279 with 50 records (~12500 tokens, total ~3487500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 280 with 50 records (~12500 tokens, total ~3500000 tokens, took 0.68s)\n",
            "Sleeping for 2.32 seconds to respect rate limit\n",
            "Upserted batch 281 with 50 records (~12500 tokens, total ~3512500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 282 with 50 records (~12500 tokens, total ~3525000 tokens, took 0.62s)\n",
            "Sleeping for 2.38 seconds to respect rate limit\n",
            "Upserted batch 283 with 50 records (~12500 tokens, total ~3537500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 284 with 50 records (~12500 tokens, total ~3550000 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 285 with 50 records (~12500 tokens, total ~3562500 tokens, took 0.67s)\n",
            "Sleeping for 2.33 seconds to respect rate limit\n",
            "Upserted batch 286 with 50 records (~12500 tokens, total ~3575000 tokens, took 0.56s)\n",
            "Sleeping for 2.44 seconds to respect rate limit\n",
            "Upserted batch 287 with 50 records (~12500 tokens, total ~3587500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 288 with 50 records (~12500 tokens, total ~3600000 tokens, took 0.77s)\n",
            "Sleeping for 2.23 seconds to respect rate limit\n",
            "Upserted batch 289 with 50 records (~12500 tokens, total ~3612500 tokens, took 0.58s)\n",
            "Sleeping for 2.42 seconds to respect rate limit\n",
            "Upserted batch 290 with 50 records (~12500 tokens, total ~3625000 tokens, took 0.66s)\n",
            "Sleeping for 2.34 seconds to respect rate limit\n",
            "Upserted batch 291 with 50 records (~12500 tokens, total ~3637500 tokens, took 0.57s)\n",
            "Sleeping for 2.43 seconds to respect rate limit\n",
            "Upserted batch 292 with 50 records (~12500 tokens, total ~3650000 tokens, took 0.78s)\n",
            "Sleeping for 2.22 seconds to respect rate limit\n",
            "Upserted batch 293 with 50 records (~12500 tokens, total ~3662500 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 294 with 50 records (~12500 tokens, total ~3675000 tokens, took 0.60s)\n",
            "Sleeping for 2.40 seconds to respect rate limit\n",
            "Upserted batch 295 with 50 records (~12500 tokens, total ~3687500 tokens, took 0.61s)\n",
            "Sleeping for 2.39 seconds to respect rate limit\n",
            "Upserted batch 296 with 50 records (~12500 tokens, total ~3700000 tokens, took 0.75s)\n",
            "Sleeping for 2.25 seconds to respect rate limit\n",
            "Upserted batch 297 with 50 records (~12500 tokens, total ~3712500 tokens, took 0.77s)\n",
            "Sleeping for 2.23 seconds to respect rate limit\n",
            "Upserted batch 298 with 50 records (~12500 tokens, total ~3725000 tokens, took 0.59s)\n",
            "Sleeping for 2.41 seconds to respect rate limit\n",
            "Upserted batch 299 with 50 records (~12500 tokens, total ~3737500 tokens, took 0.65s)\n",
            "Sleeping for 2.35 seconds to respect rate limit\n",
            "Upserted batch 300 with 40 records (~10000 tokens, total ~3747500 tokens, took 0.70s)\n",
            "Successfully upserted 14990 records into Pinecone index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI / Vector Services"
      ],
      "metadata": {
        "id": "sKniRLfsNBV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiliaze pinecone and vertex ai"
      ],
      "metadata": {
        "id": "AqCpjuhCGtoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Pinecone client\n",
        "try:\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    index = pc.Index(host=PINECONE_HOST)\n",
        "    print(\"Pinecone client initialized successfully\")\n",
        "    print(index.describe_index_stats())\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Pinecone client: {e}\")\n",
        "    raise\n",
        "\n",
        "# Initialize Vertex AI for both embedding and generative models\n",
        "\n",
        "try:\n",
        "    aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "    import vertexai\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)  # For Gemini\n",
        "    print(\"Vertex AI initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Vertex AI: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "2-ACU_wNjkb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edce0105-39c2-4c4d-b683-76f996680b13"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone client initialized successfully\n",
            "{'dimension': 768,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {},\n",
            " 'total_vector_count': 0,\n",
            " 'vector_type': 'dense'}\n",
            "Vertex AI initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings for a User Query"
      ],
      "metadata": {
        "id": "yOEwi8FaPqCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed user query with Vertex AI\n",
        "def embed_query(query, project_id, location, model_name, fallback_model=\"text-embedding-004\", task_type=\"RETRIEVAL_QUERY\"):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        for model in [model_name, fallback_model]:\n",
        "            try:\n",
        "                embedding_model = TextEmbeddingModel.from_pretrained(model)\n",
        "                embedding_input = TextEmbeddingInput(text=query, task_type=task_type)\n",
        "                for attempt in range(3):\n",
        "                    try:\n",
        "                        response = embedding_model.get_embeddings([embedding_input])\n",
        "                        embedding = response[0].values\n",
        "                        if len(embedding) != 768:\n",
        "                            print(f\"Invalid embedding dimension for model {model}: {len(embedding)}\")\n",
        "                            raise ValueError(f\"Expected 768 dimensions, got {len(embedding)}\")\n",
        "                        elapsed_time = time.time() - start_time\n",
        "                        print(f\"Embedded query '{query[:50]}...' with {model} in {elapsed_time:.2f}s\")\n",
        "                        return embedding\n",
        "                    except ResourceExhausted as e:\n",
        "                        wait_time = (2 ** attempt) * 10\n",
        "                        print(f\"Vertex AI rate limit hit for {model}: {e}. Waiting {wait_time}s.\")\n",
        "                        time.sleep(wait_time)\n",
        "                        if attempt == 2:\n",
        "                            raise\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error embedding query with {model}: {e}\")\n",
        "                        if model == fallback_model:\n",
        "                            raise\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to initialize model {model}: {e}\")\n",
        "                if model == fallback_model:\n",
        "                    raise\n",
        "                continue\n",
        "        raise ValueError(f\"Both models {model_name} and {fallback_model} failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to embed query: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "Jmj5rE4yjv6A"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Top Matches"
      ],
      "metadata": {
        "id": "5g1LnnXrQkeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search in Pinecone index\n",
        "def search_pinecone(query_embedding, namespace, top_k=5):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            namespace=namespace,\n",
        "            include_metadata=True,\n",
        "            include_values=False\n",
        "        )\n",
        "        matches = response.get(\"matches\", [])\n",
        "        results = [\n",
        "            {\n",
        "                \"id\": match[\"id\"],\n",
        "                \"text\": match[\"metadata\"].get(\"text\", \"\"),\n",
        "                \"metadata\": {k: v for k, v in match[\"metadata\"].items() if k != \"text\"},\n",
        "                \"score\": match[\"score\"]\n",
        "            }\n",
        "            for match in matches\n",
        "        ]\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Retrieved {len(results)} chunks in {elapsed_time:.2f}s\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching Pinecone: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "WkTeZBkgkCGW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Answers with Vertex AI LLM"
      ],
      "metadata": {
        "id": "AxOXsBMlQ-o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer with Vertex AI LLM\n",
        "\n",
        "def generate_answer(query, chunks, project_id, location, model_name, fallback_model=\"text-bison\", max_tokens=512):\n",
        "    \"\"\"\n",
        "    Generate answer using Vertex AI LLM (Gemini Flash 2.0).\n",
        "    Args:\n",
        "        query: User query.\n",
        "        chunks: Retrieved Pinecone chunks.\n",
        "        project_id: Google Cloud project ID.\n",
        "        location: Vertex AI region.\n",
        "        model_name: LLM model (e.g., gemini-2.0-flash).\n",
        "        fallback_model: Fallback LLM (e.g., text-bison).\n",
        "        max_tokens: Maximum output tokens.\n",
        "    Returns:\n",
        "        Generated answer string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Chunk {i+1} (Source: {chunk['metadata'].get('source_file', 'unknown')}, \"\n",
        "            f\"Title: {chunk['metadata'].get('title', 'unknown')}):\\n{chunk['text']}\"\n",
        "            for i, chunk in enumerate(chunks)\n",
        "        ])\n",
        "\n",
        "        # print(f\"context: {context}\")\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a helpful IRS tax assistant. Based on the provided context, answer the user's query accurately and concisely. \"\n",
        "            \"If the context doesn't contain relevant information about IRS or Taxes, say so and provide a general response if possible.\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            f\"Query: {query}\\n\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "\n",
        "        # Try primary model (Gemini)\n",
        "        if model_name.startswith(\"gemini\"):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                model = GenerativeModel(model_name)\n",
        "                for attempt in range(3):\n",
        "                    try:\n",
        "                        response = model.generate_content(\n",
        "                            prompt,\n",
        "                            generation_config={\n",
        "                                \"max_output_tokens\": max_tokens,\n",
        "                                \"temperature\": 0.7,\n",
        "                                \"top_p\": 0.9\n",
        "                            }\n",
        "                        )\n",
        "                        answer = response.text.strip()\n",
        "                        elapsed_time = time.time() - start_time\n",
        "                        print(f\"Generated answer with {model_name} in {elapsed_time:.2f}s\")\n",
        "                        return answer\n",
        "                    except ResourceExhausted as e:\n",
        "                        wait_time = (2 ** attempt) * 10\n",
        "                        print(f\"Vertex AI rate limit hit for {model_name}: {e}. Waiting {wait_time}s.\")\n",
        "                        time.sleep(wait_time)\n",
        "                        if attempt == 2:\n",
        "                            raise\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating answer with {model_name}: {e}\")\n",
        "                        break  # Try fallback\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to initialize {model_name}: {e}\")\n",
        "\n",
        "        # Fallback to text-bison using Predict API\n",
        "        try:\n",
        "            client = PredictionServiceClient()\n",
        "            endpoint = f\"projects/{project_id}/locations/{location}/publishers/google/models/{fallback_model}\"\n",
        "            instance = Struct()\n",
        "            instance[\"content\"] = prompt\n",
        "            start_time = time.time()\n",
        "            for attempt in range(3):\n",
        "                try:\n",
        "                    response = client.predict(\n",
        "                        endpoint=endpoint,\n",
        "                        instances=[instance],\n",
        "                        parameters={\n",
        "                            \"maxOutputTokens\": max_tokens,\n",
        "                            \"temperature\": 0.6,\n",
        "                            \"topP\": 0.7\n",
        "                        }\n",
        "                    )\n",
        "                    answer = response.predictions[0][\"content\"].strip()\n",
        "                    elapsed_time = time.time() - start_time\n",
        "                    print(f\"Generated answer with {fallback_model} in {elapsed_time:.2f}s\")\n",
        "                    return answer\n",
        "                except ResourceExhausted as e:\n",
        "                    wait_time = (2 ** attempt) * 10\n",
        "                    print(f\"Vertex AI rate limit hit for {fallback_model}: {e}. Waiting {wait_time}s.\")\n",
        "                    time.sleep(wait_time)\n",
        "                    if attempt == 2:\n",
        "                        raise\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating answer with {fallback_model}: {e}\")\n",
        "                    raise\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to initialize {fallback_model}: {e}\")\n",
        "            raise ValueError(f\"Both models {model_name} and {fallback_model} failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate answer: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "nx1Wvz-dkJWK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orchestrate the RAG Chatbot"
      ],
      "metadata": {
        "id": "r8tN9hoQRjgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG chatbot function\n",
        "def rag_chatbot(query):\n",
        "    print (f\"Sending user query ({query}) to RAG...\")\n",
        "    try:\n",
        "        print (f\"Embeding user query...\")\n",
        "        query_embedding = embed_query(query, PROJECT_ID, LOCATION, EMBEDDING_MODEL, FALLBACK_EMBEDDING_MODEL)\n",
        "\n",
        "        print (f\"Search in pinecone...\")\n",
        "        chunks = search_pinecone(query_embedding, NAMESPACE, top_k=10)\n",
        "\n",
        "        print (f\"Generating Answer...\")\n",
        "        answer = generate_answer(query, chunks, PROJECT_ID, LOCATION, LLM_MODEL, FALLBACK_LLM_MODEL)\n",
        "\n",
        "        print(f\"Input Query: {query}\\nFinal Answer: {answer}\")\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"RAG pipeline failed: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "34WKq-LhkYcV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the RAG chatbot\n",
        "def test_rag_chatbot():\n",
        "\n",
        "    test_queries = [\n",
        "        \"What do you know about IRM?\",\n",
        "        \"What is responsibilities of an SB/SE Field?\",\n",
        "        \"What is Internal Reviews staff responsibilities?\",\n",
        "        \"what is Chief Counsel coordinates?\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        try:\n",
        "            answer = rag_chatbot(query)\n",
        "            print(f\"Answer: {answer}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "test_rag_chatbot()"
      ],
      "metadata": {
        "id": "7sXHJntjkec-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7edc009-f911-4536-cfad-73478291f2d2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What do you know about IRM?\n",
            "Sending user query (What do you know about IRM?) to RAG...\n",
            "Embeding user query...\n",
            "Embedded query 'What do you know about IRM?...' with text-embedding-005 in 0.60s\n",
            "Search in pinecone...\n",
            "Retrieved 10 chunks in 0.14s\n",
            "Generating Answer...\n",
            "Generated answer with gemini-2.0-flash in 1.61s\n",
            "Input Query: What do you know about IRM?\n",
            "Final Answer: The Internal Revenue Manual (IRM) contains approved guidelines, policies, and authorities that IRS employees need to administer tax laws and other agency obligations. It explains what needs to be done, how to do it, and who is responsible, ensuring consistent work performance. The IRM is used daily by IRS employees, managers, and executives. It is also used by the GAO, Chief Counsel, Taxpayer Advocate Service, and TIGTA to ensure the IRS operates effectively, efficiently, and provides quality service, complies with laws, and reduces risk. The public can also view the IRM to learn about IRS policies and procedures. The IRM is the official source of IRS procedures, policies, directives, delegations, and guidelines. It is the foundation for fulfilling legal obligations set forth in 5 USC 552(a)(2)(c), 44 USC 3101 and 36 CFR 1222 and is an essential component for tax administration and enterprise risk management.\n",
            "Answer: The Internal Revenue Manual (IRM) contains approved guidelines, policies, and authorities that IRS employees need to administer tax laws and other agency obligations. It explains what needs to be done, how to do it, and who is responsible, ensuring consistent work performance. The IRM is used daily by IRS employees, managers, and executives. It is also used by the GAO, Chief Counsel, Taxpayer Advocate Service, and TIGTA to ensure the IRS operates effectively, efficiently, and provides quality service, complies with laws, and reduces risk. The public can also view the IRM to learn about IRS policies and procedures. The IRM is the official source of IRS procedures, policies, directives, delegations, and guidelines. It is the foundation for fulfilling legal obligations set forth in 5 USC 552(a)(2)(c), 44 USC 3101 and 36 CFR 1222 and is an essential component for tax administration and enterprise risk management.\n",
            "\n",
            "Query: What is responsibilities of an SB/SE Field?\n",
            "Sending user query (What is responsibilities of an SB/SE Field?) to RAG...\n",
            "Embeding user query...\n",
            "Embedded query 'What is responsibilities of an SB/SE Field?...' with text-embedding-005 in 0.60s\n",
            "Search in pinecone...\n",
            "Retrieved 10 chunks in 0.14s\n",
            "Generating Answer...\n",
            "Generated answer with gemini-2.0-flash in 1.54s\n",
            "Input Query: What is responsibilities of an SB/SE Field?\n",
            "Final Answer: The context provided discusses various aspects of the Small Business/Self-Employed (SB/SE) Division, including Field Examination and Field Collection. To understand the responsibilities of an SB/SE Field, it's important to differentiate between these two areas:\n",
            "\n",
            "*   **Field Examination:** This area focuses on examination case work. Managers in SB/SE Field Examination are responsible for overseeing this work.\n",
            "*   **Field Collection:** This area focuses on ensuring taxpayers understand and comply with tax laws, emphasizing payment requirements, delinquent return compliance, and delinquent prevention and investigation processing. They also plan, execute, and monitor Field Collection activities.\n",
            "\n",
            "Without more specific information about what you mean by \"SB/SE Field,\" it is difficult to provide a more precise answer.\n",
            "Answer: The context provided discusses various aspects of the Small Business/Self-Employed (SB/SE) Division, including Field Examination and Field Collection. To understand the responsibilities of an SB/SE Field, it's important to differentiate between these two areas:\n",
            "\n",
            "*   **Field Examination:** This area focuses on examination case work. Managers in SB/SE Field Examination are responsible for overseeing this work.\n",
            "*   **Field Collection:** This area focuses on ensuring taxpayers understand and comply with tax laws, emphasizing payment requirements, delinquent return compliance, and delinquent prevention and investigation processing. They also plan, execute, and monitor Field Collection activities.\n",
            "\n",
            "Without more specific information about what you mean by \"SB/SE Field,\" it is difficult to provide a more precise answer.\n",
            "\n",
            "Query: What is Internal Reviews staff responsibilities?\n",
            "Sending user query (What is Internal Reviews staff responsibilities?) to RAG...\n",
            "Embeding user query...\n",
            "Embedded query 'What is Internal Reviews staff responsibilities?...' with text-embedding-005 in 0.67s\n",
            "Search in pinecone...\n",
            "Retrieved 10 chunks in 0.14s\n",
            "Generating Answer...\n",
            "Generated answer with gemini-2.0-flash in 1.11s\n",
            "Input Query: What is Internal Reviews staff responsibilities?\n",
            "Final Answer: Based on the provided context (Chunk 1), Internal Reviews (IR) staff responsibilities include:\n",
            "\n",
            "*   Establishing and documenting the ICR program processes, policies and procedures.\n",
            "*   Collecting and analyzing data relevant to the program under review.\n",
            "*   Developing a test plan or a preliminary research memorandum as appropriate for any review, which states the objective, focus areas, related IRM references or external audits, issues identified and any additional comments important to the review.\n",
            "Answer: Based on the provided context (Chunk 1), Internal Reviews (IR) staff responsibilities include:\n",
            "\n",
            "*   Establishing and documenting the ICR program processes, policies and procedures.\n",
            "*   Collecting and analyzing data relevant to the program under review.\n",
            "*   Developing a test plan or a preliminary research memorandum as appropriate for any review, which states the objective, focus areas, related IRM references or external audits, issues identified and any additional comments important to the review.\n",
            "\n",
            "Query: what is Chief Counsel coordinates?\n",
            "Sending user query (what is Chief Counsel coordinates?) to RAG...\n",
            "Embeding user query...\n",
            "Embedded query 'what is Chief Counsel coordinates?...' with text-embedding-005 in 0.61s\n",
            "Search in pinecone...\n",
            "Retrieved 10 chunks in 0.16s\n",
            "Generating Answer...\n",
            "Generated answer with gemini-2.0-flash in 1.01s\n",
            "Input Query: what is Chief Counsel coordinates?\n",
            "Final Answer: Based on the provided context, the Office of Chief Counsel coordinates the IRS‚Äôs position in litigation with the IRS and the Department of Justice to ensure the Operating Divisions are taking consistent and appropriate technical positions in litigation. The Office also coordinates matters with other components of the Service, the Treasury Department, other government agencies, and international organizations.\n",
            "Answer: Based on the provided context, the Office of Chief Counsel coordinates the IRS‚Äôs position in litigation with the IRS and the Department of Justice to ensure the Operating Divisions are taking consistent and appropriate technical positions in litigation. The Office also coordinates matters with other components of the Service, the Treasury Department, other government agencies, and international organizations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Chatbot UI"
      ],
      "metadata": {
        "id": "nk9Slvq0SBXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chatbot\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "query_input = widgets.Text(description=\"Query:\", layout={'width': '500px'})\n",
        "output = widgets.Output()\n",
        "button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        query = query_input.value\n",
        "        if query.strip():\n",
        "            print(f\"Query: {query}\")\n",
        "            answer = rag_chatbot(query)\n",
        "            print(f\"Answer: {answer}\")\n",
        "        else:\n",
        "            print(\"Please enter a query.\")\n",
        "\n",
        "button.on_click(on_button_click)\n",
        "display(query_input, button, output)"
      ],
      "metadata": {
        "id": "jMFlf5Gvk3dk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575,
          "referenced_widgets": [
            "0ea4a863043441519320e072bd6e65f7",
            "e984b49714ff4dc5a910aef433b894c3",
            "ef1a852c81454566b7fdf97b4425fcc7",
            "265a74c3a23741aaa52864b0d8b6dc59",
            "94664d1acab14f219e4c3f7da0e64e05",
            "056a9e86bdec4434aec449ef3b34db0f",
            "f3ea92a89a3d400fa87e705e1c697095",
            "0736a3b210624dcda49f06d02306532e"
          ]
        },
        "outputId": "601ecfbf-10b7-4703-b423-7051725eaa78"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: help me with my travel to DC\n",
            "Sending user query (help me with my travel to DC) to RAG...\n",
            "Embeding user query...\n",
            "Embedded query 'help me with my travel to DC...' with text-embedding-005 in 0.58s\n",
            "Search in pinecone...\n",
            "Retrieved 10 chunks in 0.15s\n",
            "Generating Answer...\n",
            "Generated answer with gemini-2.0-flash in 2.79s\n",
            "Input Query: help me with my travel to DC\n",
            "Final Answer: To assist you with your travel to DC, here's what the provided IRS documents suggest:\n",
            "\n",
            "*   **Addressing Mail/Packages:** If you need to send mail or packages to an IRS office in the Washington, DC metro area, use the address format specified in Chunk 1. The address includes the recipient's name and title, IRS organizational symbol, building symbol and room/workstation number, 1111 Constitution Ave NW, Washington DC 20224. Chunk 1 also lists building symbols for specific locations.\n",
            "\n",
            "*   **Travel Authorization:** For official travel authorization, travel advances, travel vouchers, and travel and transportation payments, refer to IRM 1.32.1, \"IRS Local Travel Guide\" and IRM 1.32.11, \"IRS City-to-City Travel Guide\" (Chunk 7).\n",
            "\n",
            "*   **Approving Travel:** Managers have the authority to approve the use of non-contract air carriers instead of contract air carriers when justified according to the Federal Travel Regulation and IRM 1.32.11 (Chunk 4).\n",
            "\n",
            "*   **Travel Expenses:** The Operation Support manager is responsible for directing travel, approving travel expense estimates, and the expenses incurred (Chunk 3).\n",
            "\n",
            "*   **Government Vehicles:** If you are authorized to use a government vehicle, remember that you can stop for meals or other necessities along a point-to-point route (Chunk 5). Also, under certain conditions, you may be permitted to use a government vehicle to return home before traveling to a temporary duty station the following week, as governed by IRM 1.32.11 (Chunk 6). Criteria for using government vehicles and privately owned vehicles, as well as reimbursement rates, are in IRM 1.32.1 and IRM 1.32.11 (Chunk 10).\n",
            "Answer: To assist you with your travel to DC, here's what the provided IRS documents suggest:\n",
            "\n",
            "*   **Addressing Mail/Packages:** If you need to send mail or packages to an IRS office in the Washington, DC metro area, use the address format specified in Chunk 1. The address includes the recipient's name and title, IRS organizational symbol, building symbol and room/workstation number, 1111 Constitution Ave NW, Washington DC 20224. Chunk 1 also lists building symbols for specific locations.\n",
            "\n",
            "*   **Travel Authorization:** For official travel authorization, travel advances, travel vouchers, and travel and transportation payments, refer to IRM 1.32.1, \"IRS Local Travel Guide\" and IRM 1.32.11, \"IRS City-to-City Travel Guide\" (Chunk 7).\n",
            "\n",
            "*   **Approving Travel:** Managers have the authority to approve the use of non-contract air carriers instead of contract air carriers when justified according to the Federal Travel Regulation and IRM 1.32.11 (Chunk 4).\n",
            "\n",
            "*   **Travel Expenses:** The Operation Support manager is responsible for directing travel, approving travel expense estimates, and the expenses incurred (Chunk 3).\n",
            "\n",
            "*   **Government Vehicles:** If you are authorized to use a government vehicle, remember that you can stop for meals or other necessities along a point-to-point route (Chunk 5). Also, under certain conditions, you may be permitted to use a government vehicle to return home before traveling to a temporary duty station the following week, as governed by IRM 1.32.11 (Chunk 6). Criteria for using government vehicles and privately owned vehicles, as well as reimbursement rates, are in IRM 1.32.1 and IRM 1.32.11 (Chunk 10).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzGG42Xy00Fa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46UVlzwC08-D"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}
